# 🧠 Neural Network from Scratch - Forward Propagation Only

This project demonstrates how a simple feedforward neural network works, implemented entirely from scratch in Python **without using any deep learning libraries like TensorFlow or PyTorch**.

### ✅ What It Does
- Implements forward propagation step-by-step
- Uses sigmoid activation functions
- Supports multiple layers and neurons
- Demonstrates how inputs flow through a network to produce outputs

### ❌ What It Does NOT Do
- No backpropagation (i.e., the network doesn't learn from data)
- No gradient descent or weight updates

This is purely an educational project to understand the **core mechanics of the forward pass** in neural networks.

### 📌 Why This Project?
Most tutorials use high-level libraries that hide what's going on under the hood. This project is for learners who want to understand:
- How the forward pass works
- What happens layer by layer
- How activations are calculated manually

### 📄 Example Output
```python
Input: [0, 1]
Hidden Layer Output: [0.6, 0.2, ...]
Final Output: [0.83]
